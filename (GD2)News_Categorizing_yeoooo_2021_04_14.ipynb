{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "based-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-granny",
   "metadata": {},
   "source": [
    "## 로이터 뉴스 데이터 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "classified-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affected-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "normal-establishment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handed-deputy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "streaming-sierra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-contest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automotive-timeline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 클래스 빈도수:\n",
      "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
      "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
      "    42   43   44   45]\n",
      " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
      "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
      "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
      "    13   21   12   18]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"각 클래스 빈도수:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-intelligence",
   "metadata": {},
   "source": [
    "## 데이터 복원 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fluid-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respected-brook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chinese-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index + 3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spectacular-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expected-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quiet-japan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-tucson",
   "metadata": {},
   "source": [
    "## OOV , < UNK >해결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aquatic-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transaction is expected to be completed\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in [4, 587, 23, 133,6, 30, 515]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "macro-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "british-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3',\n",
       " '<sos> generale de banque sa lt <unk> br and lt heller overseas corp of chicago have each taken 50 pct stakes in <unk> company sa <unk> factors generale de banque said in a statement it gave no financial details of the transaction sa <unk> <unk> turnover in 1986 was 17 5 billion belgian francs reuter 3',\n",
       " '<sos> shr 3 28 dlrs vs 22 cts shr diluted 2 99 dlrs vs 22 cts net 46 0 mln vs 3 328 000 avg shrs 14 0 mln vs 15 2 mln year shr 5 41 dlrs vs 1 56 dlrs shr diluted 4 94 dlrs vs 1 50 dlrs net 78 2 mln vs 25 9 mln avg shrs 14 5 mln vs 15 1 mln note earnings per share reflect the two for one split effective january 6 1987 per share amounts are calculated after preferred stock dividends loss continuing operations for the qtr 1986 includes gains of sale of investments in <unk> corp of 14 mln dlrs and associated companies of 4 189 000 less writedowns of investments in national <unk> inc of 11 8 mln and <unk> corp of 15 6 mln reuter 3',\n",
       " \"<sos> the farmers home administration the u s agriculture department's farm lending arm could lose about seven billion dlrs in outstanding principal on its severely <unk> borrowers or about one fourth of its farm loan portfolio the general accounting office gao said in remarks prepared for delivery to the senate agriculture committee brian crowley senior associate director of gao also said that a preliminary analysis of proposed changes in <unk> financial eligibility standards indicated as many as one half of <unk> borrowers who received new loans from the agency in 1986 would be <unk> under the proposed system the agency has proposed evaluating <unk> credit using a variety of financial ratios instead of relying solely on <unk> ability senate agriculture committee chairman patrick leahy d vt <unk> the proposed eligibility changes telling <unk> administrator <unk> clark at a hearing that they would mark a dramatic shift in the agency's purpose away from being farmers' lender of last resort toward becoming a big city bank but clark defended the new regulations saying the agency had a responsibility to <unk> its 70 billion dlr loan portfolio in a <unk> yet <unk> manner crowley of gao <unk> <unk> arm said the proposed credit <unk> system attempted to ensure that <unk> would make loans only to borrowers who had a reasonable change of repaying their debt reuter 3\",\n",
       " '<sos> seton co said its board has received a proposal from chairman and chief executive officer philip d <unk> to acquire seton for 15 75 dlrs per share in cash seton said the acquisition bid is subject to <unk> arranging the necessary financing it said he intends to ask other members of senior management to participate the company said <unk> owns 30 pct of seton stock and other management members another 7 5 pct seton said it has formed an independent board committee to consider the offer and has deferred the annual meeting it had scheduled for march 31 reuter 3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "organizational-framing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]),\n",
       "       list([1, 2768, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 1315, 19, 12, 11, 82, 52, 29, 283, 1135, 558, 2, 265, 2, 6607, 8, 6607, 118, 371, 10, 1503, 281, 4, 143, 4811, 760, 50, 2088, 225, 139, 683, 4, 48, 193, 862, 41, 967, 1999, 30, 1086, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 6273, 13, 260, 951, 6607, 8, 69, 1749, 18, 82, 41, 30, 306, 3342, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 1721, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 3825, 2, 2476, 6, 3684, 6940, 4, 1126, 41, 263, 84, 395, 649, 18, 82, 838, 1317, 4, 572, 4, 106, 13, 25, 595, 2445, 40, 85, 7369, 518, 5, 4, 1126, 51, 115, 680, 16, 6, 719, 250, 27, 429, 6607, 8, 6940, 114, 343, 84, 142, 20, 5, 1145, 1538, 4, 65, 494, 474, 27, 69, 445, 11, 1816, 6607, 8, 109, 181, 2768, 2, 62, 1810, 6, 624, 901, 6940, 107, 4, 1126, 34, 524, 4, 6940, 1126, 41, 447, 7, 1427, 13, 69, 251, 18, 872, 876, 1539, 468, 9063, 242, 5, 646, 27, 1888, 169, 283, 87, 9, 10, 2, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2333, 6, 195, 7254, 6337, 268, 609, 4, 195, 41, 1017, 2765, 2, 4, 73, 706, 2, 92, 4, 91, 3917, 36, 8, 51, 144, 23, 1858, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2233, 7411, 269, 274, 87, 6607, 8, 787, 283, 34, 596, 661, 5467, 13, 2362, 1816, 90, 2, 84, 22, 2202, 1816, 54, 748, 6607, 8, 87, 62, 6154, 84, 161, 5, 1208, 480, 4, 2, 416, 6, 538, 122, 115, 55, 129, 1104, 1445, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 2, 1507, 64, 8862, 22, 125, 2, 9, 2876, 172, 399, 9, 2, 5206, 9, 2, 122, 36, 8, 6642, 172, 247, 100, 97, 6940, 34, 75, 477, 541, 4, 283, 182, 4, 2, 295, 301, 2, 125, 2, 6607, 8, 77, 57, 445, 283, 1998, 217, 31, 380, 704, 51, 77, 2, 509, 5, 476, 9, 2876, 122, 115, 853, 6, 1061, 52, 10, 2, 2, 1308, 5, 4, 283, 182, 36, 8, 5296, 114, 30, 531, 6, 6376, 9, 2470, 529, 13, 2, 2, 58, 529, 7, 2148, 2, 185, 1028, 240, 5296, 1028, 949, 657, 57, 6, 1046, 283, 36, 8, 6607, 8, 4, 2217, 34, 9177, 13, 10, 4910, 5, 4, 141, 283, 120, 50, 2877, 7, 1049, 43, 10, 181, 283, 734, 115, 55, 3356, 476, 6, 2195, 10, 73, 120, 50, 41, 6877, 169, 87, 6607, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2409, 30, 1888, 1171, 161, 4, 294, 517, 23, 2, 25, 398, 9, 2060, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 1569, 28, 69, 61, 376, 514, 90, 1249, 62, 2, 13, 4, 2217, 696, 122, 404, 2936, 22, 134, 6, 187, 514, 10, 1249, 107, 4, 96, 1043, 1569, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 1249, 27, 4, 153, 5, 52, 29, 36, 8, 6607, 8, 612, 408, 10, 3133, 283, 76, 27, 1504, 31, 169, 951, 2, 122, 36, 8, 283, 236, 62, 641, 84, 618, 2, 22, 8417, 8409, 9, 274, 7322, 399, 7587, 51, 115, 55, 45, 4044, 31, 4, 490, 558, 36, 8, 224, 2, 115, 57, 85, 1655, 2671, 5, 283, 6, 4, 37, 38, 7, 1797, 185, 77, 4446, 4, 555, 298, 77, 240, 2, 7, 327, 652, 194, 8773, 6233, 34, 2, 5463, 4884, 1297, 6, 240, 260, 458, 87, 6, 134, 514, 10, 1249, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 8577, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2217, 6, 2172, 4278, 434, 835, 22, 3598, 3746, 434, 835, 7, 48, 6607, 8, 618, 225, 586, 333, 122, 572, 126, 2768, 1998, 62, 133, 6, 2458, 233, 28, 602, 188, 5, 4, 704, 1998, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 1732, 1516, 281, 4, 225, 760, 17, 12]),\n",
       "       list([1, 4, 309, 2276, 4759, 5, 2015, 403, 1920, 33, 1575, 1627, 1173, 87, 13, 536, 78, 6490, 399, 7, 2068, 212, 10, 634, 179, 8, 137, 5602, 7, 2775, 33, 30, 1015, 43, 33, 5602, 50, 489, 4, 403, 6, 96, 399, 7, 1953, 3587, 8427, 6603, 4132, 3669, 8180, 7163, 9, 2015, 8, 2, 2, 1683, 791, 5, 740, 220, 707, 13, 4, 634, 634, 54, 1405, 6331, 4, 361, 182, 24, 511, 972, 137, 403, 1920, 529, 6, 96, 3711, 399, 41, 30, 2776, 21, 10, 8491, 2002, 503, 5, 188, 6, 353, 26, 2474, 21, 432, 4, 4234, 23, 3288, 435, 34, 737, 6, 246, 7528, 274, 1173, 1627, 87, 13, 399, 992, 27, 274, 403, 87, 2631, 85, 480, 52, 2015, 403, 820, 13, 10, 139, 9, 115, 949, 609, 890, 819, 6, 812, 593, 7, 576, 7, 194, 2329, 216, 2, 8, 2, 8, 634, 33, 768, 2085, 593, 4, 403, 1920, 185, 9, 107, 403, 87, 2, 107, 1635, 410, 4, 682, 189, 161, 1635, 762, 274, 5319, 115, 30, 43, 389, 410, 4, 682, 107, 1635, 762, 456, 36, 8, 184, 4057, 95, 1854, 107, 403, 87, 302, 2, 8, 129, 100, 756, 7, 3288, 96, 298, 55, 370, 731, 866, 189, 115, 949, 9695, 115, 949, 343, 756, 2, 9, 115, 949, 343, 756, 2509, 36, 8, 17, 12]),\n",
       "       list([1, 2, 2, 65, 9, 249, 1096, 8, 16, 515, 4, 211, 5, 881, 7, 78, 181, 65, 9, 249, 1441, 6, 56, 387, 280, 141, 81, 13, 360, 11, 15, 4, 49, 8, 16, 385, 69, 68, 327, 5, 25, 132, 20, 128, 7, 4, 6452, 3254, 9, 681, 2, 1441, 25, 78, 473, 814, 1110, 16, 8, 16, 529, 69, 119, 11, 15, 5, 4, 1154, 6, 2, 1679, 21, 25, 645, 4005, 975, 2170, 303, 1679, 464, 21, 4, 1004, 279, 11, 15, 5, 975, 57, 85, 2589, 457, 207, 448, 43, 10, 447, 16, 8, 4, 49, 8, 4, 123, 755, 62, 4198, 6, 960, 42, 2559, 5, 126, 231, 8326, 9, 42, 148, 5, 128, 6, 165, 44, 20, 22, 165, 70, 20, 457, 207, 1504, 16, 8, 4, 2, 2049, 96, 13, 19, 1340, 18, 1379, 34, 75, 717, 6, 132, 26, 22, 19, 132, 15, 4, 49, 8, 387, 280, 200, 6, 67, 4, 404, 5, 879, 122, 28, 4, 6452, 3254, 979, 17, 12]),\n",
       "       list([1, 470, 354, 2, 4231, 62, 2373, 509, 1687, 5138, 7, 4, 4893, 410, 4, 8178, 9, 2185, 9371, 7, 68, 5, 4, 2414, 9676, 1025, 7, 4, 4051, 13, 509, 206, 4, 8178, 166, 5, 4821, 8, 7, 5271, 9, 2774, 84, 6, 132, 1687, 62, 257, 6, 30, 8908, 7, 4, 4893, 9, 657, 4, 473, 5, 4, 2, 2, 62, 1465, 2125, 7, 4897, 250, 6, 4, 2, 1094, 2, 173, 8, 290, 9676, 662, 7, 4, 1033, 4051, 28, 4, 460, 65, 1843, 5, 2, 9, 2, 64, 2243, 77, 8, 859, 173, 7, 7212, 2774, 8, 4, 9676, 662, 7, 4, 4051, 62, 4, 2414, 13, 124, 206, 31, 1094, 4898, 10, 3274, 2598, 6, 933, 1564, 7, 4, 2308, 9334, 5, 4, 403, 5, 2, 50, 2, 5271, 9, 2774, 4, 4893, 23, 84, 6, 68, 2, 9764, 31, 2, 9, 2, 8030, 16, 172, 967, 2, 9652, 109, 4165, 274, 2185, 2, 173, 8, 859, 1190, 452, 4231, 91, 1671, 281, 4, 1438, 51, 10, 226, 499, 7, 9064, 114, 1099, 871, 6, 10, 4748, 4, 173, 8, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-french",
   "metadata": {},
   "source": [
    "## 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "european-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infectious-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(train, test):\n",
    "    dtmvector = CountVectorizer()\n",
    "    train_dtm = dtmvector.fit_transform(train)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(train_dtm)\n",
    "    \n",
    "    test_dtm = dtmvector.transform(test) #테스트 데이터를 DTM으로 변환\n",
    "    tfidfv_test = tfidf_transformer.transform(test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "    return tfidfv, tfidfv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-seattle",
   "metadata": {},
   "source": [
    "## 단어 5천개 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tired-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train_5000, y_train_5000), (x_test_5000, y_test_5000) = reuters.load_data(num_words = 5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "alternative-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_5000)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_5000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_5000 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test_5000)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_5000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_5000 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "loved-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidfv, tfidfv_test) = tfidf(x_train_5000,x_test_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "genetic-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train_5000, x_test_5000\n",
    "y_train, y_test = y_train_5000, y_test_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "imposed-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.6731967943009796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n",
      "CNB 정확도: 0.7707034728406055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        12\n",
      "           1       0.63      0.86      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.91      0.89      0.90       813\n",
      "           4       0.74      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.86      0.86        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.57      0.21      0.31        38\n",
      "           9       0.82      0.92      0.87        25\n",
      "          10       0.96      0.80      0.87        30\n",
      "          11       0.54      0.76      0.63        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.69      0.59      0.64        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.67      0.79      0.72        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.56      0.80      0.66       133\n",
      "          20       0.79      0.33      0.46        70\n",
      "          21       0.78      0.67      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.67      0.33      0.44        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.86      0.77      0.81        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.63      0.44      0.48      2246\n",
      "weighted avg       0.76      0.77      0.75      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.8058771148708815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.77      0.80      0.79       105\n",
      "           2       0.74      0.85      0.79        20\n",
      "           3       0.91      0.93      0.92       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.64      0.74      0.68        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.64      0.73      0.68        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.64      0.62      0.63        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.83      0.56      0.67         9\n",
      "          16       0.67      0.73      0.70        99\n",
      "          17       0.82      0.75      0.78        12\n",
      "          18       0.80      0.60      0.69        20\n",
      "          19       0.66      0.68      0.67       133\n",
      "          20       0.61      0.47      0.53        70\n",
      "          21       0.62      0.78      0.69        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.55      0.50      0.52        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.67      0.40      0.50        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.42      0.59        12\n",
      "          31       0.70      0.54      0.61        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.38      0.27      0.32        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.40      0.40      0.40         5\n",
      "          40       0.75      0.30      0.43        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.73      0.61      0.64      2246\n",
      "weighted avg       0.80      0.81      0.80      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7666963490650045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.72      0.71      0.72       105\n",
      "           2       0.74      0.70      0.72        20\n",
      "           3       0.90      0.91      0.90       813\n",
      "           4       0.79      0.83      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.80      0.86      0.83        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.56      0.66      0.60        38\n",
      "           9       0.73      0.76      0.75        25\n",
      "          10       0.74      0.77      0.75        30\n",
      "          11       0.60      0.73      0.66        83\n",
      "          12       0.23      0.23      0.23        13\n",
      "          13       0.59      0.54      0.56        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.65      0.71      0.68        99\n",
      "          17       0.80      0.33      0.47        12\n",
      "          18       0.85      0.55      0.67        20\n",
      "          19       0.64      0.63      0.63       133\n",
      "          20       0.53      0.51      0.52        70\n",
      "          21       0.58      0.70      0.63        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.56      0.42      0.48        12\n",
      "          24       0.53      0.47      0.50        19\n",
      "          25       0.84      0.52      0.64        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.27      0.30      0.29        10\n",
      "          29       0.38      0.75      0.50         4\n",
      "          30       0.78      0.58      0.67        12\n",
      "          31       0.71      0.38      0.50        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.44      0.36      0.40        11\n",
      "          37       0.40      1.00      0.57         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.60      0.30      0.40        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.75      1.00      0.86         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.65      0.57      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "의사결정 트리 정확도: 0.6179875333926982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.72      0.40      0.52       105\n",
      "           2       0.60      0.45      0.51        20\n",
      "           3       0.94      0.84      0.89       813\n",
      "           4       0.39      0.91      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.57      0.73        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.88      0.88      0.88        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.48      0.54        83\n",
      "          12       0.17      0.08      0.11        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.82      0.69        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.62      0.26      0.37       133\n",
      "          20       0.33      0.03      0.05        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.86      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.24      0.17      0.18      2246\n",
      "weighted avg       0.61      0.62      0.57      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도: 0.701246660730187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.33        12\n",
      "           1       0.42      0.78      0.55       105\n",
      "           2       0.44      0.35      0.39        20\n",
      "           3       0.84      0.90      0.87       813\n",
      "           4       0.68      0.84      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.86      0.43      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.59      0.53      0.56        38\n",
      "           9       0.71      0.40      0.51        25\n",
      "          10       0.89      0.53      0.67        30\n",
      "          11       0.57      0.69      0.62        83\n",
      "          12       0.33      0.15      0.21        13\n",
      "          13       0.46      0.32      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       1.00      0.11      0.20         9\n",
      "          16       0.70      0.67      0.68        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.60      0.45      0.51        20\n",
      "          19       0.62      0.64      0.63       133\n",
      "          20       0.46      0.33      0.38        70\n",
      "          21       0.65      0.41      0.50        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.75      0.25      0.38        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       0.87      0.42      0.57        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.20      0.33         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.33      0.09      0.14        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.25      0.12      0.17         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.54      0.31      0.36      2246\n",
      "weighted avg       0.69      0.70      0.68      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래디언트 부스팅 트리 정확도: 0.767586821015138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.80      0.68      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.90      0.90      0.90       813\n",
      "           4       0.76      0.83      0.79       474\n",
      "           5       0.14      0.20      0.17         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.64      0.66      0.65        38\n",
      "           9       0.91      0.84      0.87        25\n",
      "          10       0.87      0.87      0.87        30\n",
      "          11       0.62      0.66      0.64        83\n",
      "          12       0.46      0.46      0.46        13\n",
      "          13       0.55      0.43      0.48        37\n",
      "          14       0.08      0.50      0.14         2\n",
      "          15       0.33      0.22      0.27         9\n",
      "          16       0.72      0.77      0.75        99\n",
      "          17       0.33      0.33      0.33        12\n",
      "          18       0.61      0.55      0.58        20\n",
      "          19       0.71      0.65      0.68       133\n",
      "          20       0.56      0.44      0.50        70\n",
      "          21       0.67      0.67      0.67        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.36      0.42      0.38        12\n",
      "          24       0.71      0.63      0.67        19\n",
      "          25       0.91      0.65      0.75        31\n",
      "          26       0.75      0.75      0.75         8\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.22      0.50      0.31         4\n",
      "          30       0.38      0.42      0.40        12\n",
      "          31       0.60      0.46      0.52        13\n",
      "          32       0.88      0.70      0.78        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       0.50      0.29      0.36         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.67      0.55      0.60        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.25      0.33      0.29         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.71      0.50      0.59        10\n",
      "          41       0.44      0.50      0.47         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.50      0.67      0.57         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.60      0.59      0.58      2246\n",
      "weighted avg       0.77      0.77      0.77      2246\n",
      "\n",
      "보팅 정확도: 0.8161175422974176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.71      0.85      0.77        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.72      0.68      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.93      0.90      0.92        30\n",
      "          11       0.67      0.70      0.68        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       0.12      0.50      0.20         2\n",
      "          15       0.67      0.44      0.53         9\n",
      "          16       0.74      0.74      0.74        99\n",
      "          17       0.57      0.67      0.62        12\n",
      "          18       0.72      0.65      0.68        20\n",
      "          19       0.73      0.68      0.71       133\n",
      "          20       0.61      0.49      0.54        70\n",
      "          21       0.66      0.78      0.71        27\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.75      0.63      0.69        19\n",
      "          25       0.96      0.74      0.84        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.44      0.40      0.42        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.62      0.42      0.50        12\n",
      "          31       0.75      0.69      0.72        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.71      1.00      0.83         5\n",
      "          34       1.00      0.43      0.60         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.45      0.45      0.45        11\n",
      "          37       0.67      1.00      0.80         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       0.80      0.40      0.53        10\n",
      "          41       0.67      0.50      0.57         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       0.71      0.83      0.77         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.71      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"나이브 베이즈 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "\n",
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"CNB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "\n",
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"로지스틱 회귀 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"SVC 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"의사결정 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"랜덤 포레스트 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "\n",
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"그래디언트 부스팅 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"보팅 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ahead-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.50      0.80      0.62       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.86      0.89      0.87       813\n",
      "           4       0.59      0.95      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.48      0.73      0.58        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       1.00      0.14      0.24        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.60      0.66      0.62        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.51      0.81      0.63       133\n",
      "          20       0.90      0.13      0.23        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.06      0.12        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67      2246\n",
      "   macro avg       0.16      0.12      0.11      2246\n",
      "weighted avg       0.60      0.67      0.60      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, mod.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-basics",
   "metadata": {},
   "source": [
    "## 모든 단어 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fitting-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = reuters.load_data(test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "turkish-bidder",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_all)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_all[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_all = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test_all)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_all[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_all = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "described-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidfv, tfidfv_test) = tfidf(x_train_all,x_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "published-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train_all, x_test_all\n",
    "y_train, y_test = y_train_all, y_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "packed-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.5997328584149599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.18      0.29        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.73      0.58      0.64       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.50      2246\n",
      "\n",
      "CNB 정확도: 0.7649154051647373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.813446126447017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.80      0.77       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      1.00      0.97        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.61      0.62      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.67      0.50      0.57        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.69      0.70      0.69       133\n",
      "          20       0.60      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.75      0.30      0.43        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.89      0.67      0.76        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.42      0.45      0.43        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.76      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7773820124666073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.69      0.70      0.69       105\n",
      "           2       0.64      0.70      0.67        20\n",
      "           3       0.91      0.91      0.91       813\n",
      "           4       0.81      0.85      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.63      0.71      0.67        38\n",
      "           9       1.00      0.84      0.91        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.63      0.75      0.69        83\n",
      "          12       0.27      0.23      0.25        13\n",
      "          13       0.56      0.51      0.54        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.33      0.11      0.17         9\n",
      "          16       0.61      0.72      0.66        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.92      0.55      0.69        20\n",
      "          19       0.66      0.65      0.65       133\n",
      "          20       0.49      0.44      0.47        70\n",
      "          21       0.59      0.81      0.69        27\n",
      "          22       0.50      0.29      0.36         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.85      0.71      0.77        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.86      0.46      0.60        13\n",
      "          32       0.78      0.70      0.74        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.33      0.20      0.25        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.57      0.80      0.67         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.63      0.56      0.58      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n",
      "의사결정 트리 정확도: 0.6211041852181657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.69      0.43      0.53       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.83      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.41      0.50       133\n",
      "          20       0.83      0.07      0.13        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.15      0.15      2246\n",
      "weighted avg       0.62      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도: 0.6544968833481746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래디언트 부스팅 트리 정확도: 0.7702582368655387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.81      0.71      0.76       105\n",
      "           2       0.58      0.70      0.64        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.63      0.62        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.65      0.63        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.48      0.32      0.39        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.72      0.71      0.71        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.71      0.64      0.67       133\n",
      "          20       0.64      0.41      0.50        70\n",
      "          21       0.61      0.63      0.62        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.50      0.54      0.52        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.50      0.64      0.56        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.83      0.50      0.62        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.57      0.57      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "보팅 정확도: 0.8187889581478184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.67      0.71      0.69        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.73      0.76      0.74        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.66      0.50      0.57        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.73      0.58      0.65        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      1.00      0.67         4\n",
      "          30       0.54      0.58      0.56        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.40      0.57        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.73      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"나이브 베이즈 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "\n",
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"CNB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "\n",
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"로지스틱 회귀 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"SVC 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"의사결정 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"랜덤 포레스트 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "\n",
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"그래디언트 부스팅 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"보팅 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-comparative",
   "metadata": {},
   "source": [
    "## 단어 3개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tracked-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train_3, y_train_3), (x_test_3, y_test_3) = reuters.load_data(num_words = 3, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "central-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_3)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_3[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_3 = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test_3)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_3[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_3 = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fatty-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tfidfv, tfidfv_test) = tfidf(x_train_all,x_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "raising-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train_3, x_test_3\n",
    "y_train, y_test = y_train_3, y_test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sealed-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.5997328584149599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.79      0.21      0.33       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.72      0.92      0.81       813\n",
      "           4       0.45      0.96      0.61       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.80      0.29      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.75      0.18      0.29        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.73      0.58      0.64       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60      2246\n",
      "   macro avg       0.09      0.07      0.07      2246\n",
      "weighted avg       0.50      0.60      0.50      2246\n",
      "\n",
      "CNB 정확도: 0.7649154051647373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.813446126447017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.75      0.80      0.77       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.93      0.93      0.93       813\n",
      "           4       0.81      0.87      0.84       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.93      1.00      0.97        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.68      0.71      0.69        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.93      0.87      0.90        30\n",
      "          11       0.66      0.73      0.70        83\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.61      0.62      0.61        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.71      0.56      0.63         9\n",
      "          16       0.71      0.77      0.74        99\n",
      "          17       0.67      0.50      0.57        12\n",
      "          18       0.76      0.65      0.70        20\n",
      "          19       0.69      0.70      0.69       133\n",
      "          20       0.60      0.49      0.54        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.69      0.75      0.72        12\n",
      "          24       0.62      0.53      0.57        19\n",
      "          25       0.92      0.74      0.82        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.75      0.30      0.43        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       0.89      0.67      0.76        12\n",
      "          31       0.75      0.46      0.57        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.29      0.44         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.42      0.45      0.43        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.81      2246\n",
      "   macro avg       0.76      0.64      0.67      2246\n",
      "weighted avg       0.81      0.81      0.81      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7773820124666073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.69      0.70      0.69       105\n",
      "           2       0.64      0.70      0.67        20\n",
      "           3       0.91      0.91      0.91       813\n",
      "           4       0.81      0.85      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.63      0.71      0.67        38\n",
      "           9       1.00      0.84      0.91        25\n",
      "          10       0.89      0.83      0.86        30\n",
      "          11       0.63      0.75      0.69        83\n",
      "          12       0.27      0.23      0.25        13\n",
      "          13       0.56      0.51      0.54        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.33      0.11      0.17         9\n",
      "          16       0.61      0.72      0.66        99\n",
      "          17       1.00      0.25      0.40        12\n",
      "          18       0.92      0.55      0.69        20\n",
      "          19       0.66      0.65      0.65       133\n",
      "          20       0.49      0.44      0.47        70\n",
      "          21       0.59      0.81      0.69        27\n",
      "          22       0.50      0.29      0.36         7\n",
      "          23       0.57      0.67      0.62        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.85      0.71      0.77        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.86      0.46      0.60        13\n",
      "          32       0.78      0.70      0.74        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.57      0.57      0.57         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.33      0.27      0.30        11\n",
      "          37       0.33      0.50      0.40         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.50      0.40      0.44         5\n",
      "          40       0.33      0.20      0.25        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.60      1.00      0.75         6\n",
      "          44       0.57      0.80      0.67         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78      2246\n",
      "   macro avg       0.63      0.56      0.58      2246\n",
      "weighted avg       0.78      0.78      0.77      2246\n",
      "\n",
      "의사결정 트리 정확도: 0.6211041852181657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.69      0.43      0.53       105\n",
      "           2       0.75      0.45      0.56        20\n",
      "           3       0.94      0.85      0.89       813\n",
      "           4       0.40      0.89      0.55       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       1.00      0.16      0.28        25\n",
      "          10       0.89      0.80      0.84        30\n",
      "          11       0.58      0.60      0.59        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.61      0.83      0.70        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.67      0.41      0.50       133\n",
      "          20       0.83      0.07      0.13        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.67      0.11      0.18        19\n",
      "          25       0.60      0.19      0.29        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2246\n",
      "   macro avg       0.22      0.15      0.15      2246\n",
      "weighted avg       0.62      0.62      0.58      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도: 0.6544968833481746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35        12\n",
      "           1       0.35      0.60      0.44       105\n",
      "           2       0.32      0.40      0.36        20\n",
      "           3       0.82      0.89      0.85       813\n",
      "           4       0.62      0.84      0.71       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.67      0.43      0.52        14\n",
      "           7       0.50      0.33      0.40         3\n",
      "           8       0.51      0.47      0.49        38\n",
      "           9       1.00      0.28      0.44        25\n",
      "          10       0.46      0.20      0.28        30\n",
      "          11       0.56      0.64      0.60        83\n",
      "          12       0.40      0.15      0.22        13\n",
      "          13       0.33      0.16      0.22        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.59      0.46      0.52        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.44      0.20      0.28        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.51      0.33      0.40        70\n",
      "          21       0.55      0.22      0.32        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.33      0.08      0.13        12\n",
      "          24       0.33      0.05      0.09        19\n",
      "          25       1.00      0.23      0.37        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.10      0.18        10\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.50      0.60         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.65      2246\n",
      "   macro avg       0.40      0.25      0.28      2246\n",
      "weighted avg       0.63      0.65      0.62      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래디언트 부스팅 트리 정확도: 0.7702582368655387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.81      0.71      0.76       105\n",
      "           2       0.58      0.70      0.64        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.77      0.71      0.74        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.60      0.63      0.62        38\n",
      "           9       0.91      0.80      0.85        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.65      0.63        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.48      0.32      0.39        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.72      0.71      0.71        99\n",
      "          17       0.83      0.42      0.56        12\n",
      "          18       0.59      0.50      0.54        20\n",
      "          19       0.71      0.64      0.67       133\n",
      "          20       0.64      0.41      0.50        70\n",
      "          21       0.61      0.63      0.62        27\n",
      "          22       0.33      0.14      0.20         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.69      0.47      0.56        19\n",
      "          25       0.83      0.65      0.73        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.33      0.50      0.40         4\n",
      "          28       0.25      0.20      0.22        10\n",
      "          29       0.43      0.75      0.55         4\n",
      "          30       0.36      0.42      0.38        12\n",
      "          31       0.50      0.54      0.52        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       0.33      0.17      0.22         6\n",
      "          36       0.50      0.64      0.56        11\n",
      "          37       0.50      1.00      0.67         2\n",
      "          38       0.33      0.33      0.33         3\n",
      "          39       0.33      0.20      0.25         5\n",
      "          40       0.83      0.50      0.62        10\n",
      "          41       0.62      0.62      0.62         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.43      0.50      0.46         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.62      0.57      0.57      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "보팅 정확도: 0.8187889581478184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        12\n",
      "           1       0.80      0.77      0.79       105\n",
      "           2       0.67      0.80      0.73        20\n",
      "           3       0.93      0.94      0.93       813\n",
      "           4       0.82      0.88      0.85       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.87      0.93      0.90        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.80      0.80      0.80        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.67      0.71      0.69        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.29      1.00      0.44         2\n",
      "          15       0.40      0.22      0.29         9\n",
      "          16       0.73      0.76      0.74        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.71      0.71      0.71       133\n",
      "          20       0.66      0.50      0.57        70\n",
      "          21       0.63      0.81      0.71        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.62      0.67      0.64        12\n",
      "          24       0.73      0.58      0.65        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       0.67      0.50      0.57         4\n",
      "          28       0.33      0.30      0.32        10\n",
      "          29       0.50      1.00      0.67         4\n",
      "          30       0.54      0.58      0.56        12\n",
      "          31       0.82      0.69      0.75        13\n",
      "          32       1.00      1.00      1.00        10\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.80      0.57      0.67         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.54      0.64      0.58        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.25      0.20      0.22         5\n",
      "          40       1.00      0.40      0.57        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.83      0.83      0.83         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.82      2246\n",
      "   macro avg       0.73      0.66      0.66      2246\n",
      "weighted avg       0.82      0.82      0.81      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = MultinomialNB()\n",
    "mod.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"나이브 베이즈 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, mod.predict(tfidfv_test)))\n",
    "\n",
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"CNB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, cb.predict(tfidfv_test)))\n",
    "\n",
    "lr = LogisticRegression(C=10000, penalty='l2')\n",
    "lr.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"로지스틱 회귀 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lr.predict(tfidfv_test)))\n",
    "\n",
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"SVC 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, lsvc.predict(tfidfv_test)))\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "tree.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"의사결정 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, tree.predict(tfidfv_test)))\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"랜덤 포레스트 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, forest.predict(tfidfv_test)))\n",
    "\n",
    "grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "grbt.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"그래디언트 부스팅 트리 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, grbt.predict(tfidfv_test)))\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "], voting='soft', n_jobs=-1)\n",
    "voting_classifier.fit(tfidfv, y_train)\n",
    "\n",
    "predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"보팅 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "print(classification_report(y_test, voting_classifier.predict(tfidfv_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-stranger",
   "metadata": {},
   "source": [
    "## DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "seven-eleven",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words = 5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "danish-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_MLPC = decoded\n",
    "\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_MLPC = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "noticed-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(100, 100), random_state=100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "(tfidfv, tfidfv_test) = tfidf(x_train_MLPC,x_test_MLPC)\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(100, 100), random_state=100)\n",
    "clf.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "respected-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70        12\n",
      "           1       0.77      0.70      0.73       105\n",
      "           2       0.70      0.70      0.70        20\n",
      "           3       0.91      0.92      0.92       813\n",
      "           4       0.78      0.88      0.83       474\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.76      0.88      0.81        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.65      0.80      0.72        83\n",
      "          12       0.38      0.23      0.29        13\n",
      "          13       0.61      0.59      0.60        37\n",
      "          14       0.50      0.50      0.50         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.70      0.72      0.71        99\n",
      "          17       0.50      0.25      0.33        12\n",
      "          18       0.86      0.60      0.71        20\n",
      "          19       0.65      0.69      0.67       133\n",
      "          20       0.64      0.59      0.61        70\n",
      "          21       0.73      0.70      0.72        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.44      0.33      0.38        12\n",
      "          24       0.69      0.58      0.63        19\n",
      "          25       0.95      0.68      0.79        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.80      1.00      0.89         4\n",
      "          28       0.36      0.40      0.38        10\n",
      "          29       0.57      1.00      0.73         4\n",
      "          30       1.00      0.58      0.74        12\n",
      "          31       0.88      0.54      0.67        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       0.75      0.43      0.55         7\n",
      "          35       1.00      0.33      0.50         6\n",
      "          36       0.71      0.45      0.56        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.67      0.20      0.31        10\n",
      "          41       0.43      0.38      0.40         8\n",
      "          42       1.00      1.00      1.00         3\n",
      "          43       0.67      1.00      0.80         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.70      0.60      0.63      2246\n",
      "weighted avg       0.79      0.80      0.79      2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aiffel-dj16/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(tfidfv_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
