{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = os.getenv('HOME') +'/aiffel/s2s_translation'\n",
    "path_to_file_ko = path_to_dir +'/korean-english-park.train/korean-english-park.train.ko'\n",
    "path_to_file_en = path_to_dir +'/korean-english-park.train/korean-english-park.train.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size:  94123\n",
      "Example: \n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file_ko, \"r\") as f:\n",
    "    raw_ko = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size: \", len(raw_ko))\n",
    "print(\"Example: \")\n",
    "\n",
    "for sen in raw_ko[0:100][::20]:\n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size:  94123\n",
      "Example: \n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file_en, \"r\") as f:\n",
    "    raw_en = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size: \", len(raw_en))\n",
    "print(\"Example: \")\n",
    "\n",
    "for sen in raw_en[0:100][::20]:\n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = zip(raw_ko, raw_en)\n",
    "cleaned_corpus = set(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    #한글 포함\n",
    "    sentence = re.sub(r\"[^a-zA-Z|0-9|ㄱ-하-ㅣ가-힣?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "        \n",
    "    if s_token == False:\n",
    "        sentence = mecab.morphs(sentence)\n",
    "    else:\n",
    "        sentence = sentence.split()\n",
    "        \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = list(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이형택은 프랑스의 줄리앙 베네토에 2-1(2-66-26-3) 역전승을 거뒀다.',\n",
       "  'He closed out the set in the next game with one of his 23 aces.'),\n",
       " ('이스라엘 문화재청은 21일(현지시간) 예루살렘 국립공원에서 고대 금화 264개가 발견됐다고 보고했다.',\n",
       "  'the discovery of 264 ancient gold coins in Jerusalem National Park.'),\n",
       " ('조계종은, 불법적인 정부 보조금과 기타 혜택에 대한 대가로 조계사에서 신씨에게 뇌물을 공여했다고 조선일보가 거짓 보도를 했다고 주장했다.',\n",
       "  'The religious group claimed the Chosun Ilbo falsely accused a Jogye Order temple of paying Shin in return for illegal government subsidies and other benefits.'),\n",
       " ('하나는 요추 아랫부분에 여자의 쐐기골(V자) 모양의 등뼈가 남자들에 비해 더 네모난 모양이라는 점이다.',\n",
       "  'One lower lumbar vertebra is wedged-shaped in women and more square in men;'),\n",
       " ('그는 이날 워싱턴주 아메리칸 대학에 대규모로 모인 학생들 앞 에서 조카인 캐롤라인 케네디, 아들인 패트릭 케네디 하원의원과 함께 오바바 지지선언을 했다.',\n",
       "  'Patrick Kennedy, and his niece, Caroline Kennedy, before a screaming capacity crowd of students at American University in Washington.'),\n",
       " ('호랑이와 표범 등 고양이과 동물들을 우리가 새로 만들어질 때까지 당분간 실내에서 생활한다.',\n",
       "  \"The zoo's large cats will be kept indoors until workers complete the improvements to their enclosure.\"),\n",
       " ('이라크 군이 석유절도를 막기 위해 이전 석유 밀매업자에게 협조했던 부족 청년들을 대상으로 신병을 모집하고 있다고 이라크 석유장관이 밝혔다.',\n",
       "  \"BAGHDAD, Iraq (CNN) Iraqi forces fighting oil theft have been working to recruit young men from tribes that long have been aiding the smugglers, the country's oil minister told CNN.\"),\n",
       " ('FBI는 쿠퍼가 자신이 가고자 하는 곳을 정확히 정하지 않았으며 자신이 어디서 뛰어 내렸는지에 대해서 도 알지 못했을 것이라고 덧붙였다.',\n",
       "  \"He hadn't specified a route for the plane to fly and had no way of knowing where he was when he went out the exit.\"),\n",
       " ('코리아타임즈에따르면 교육부는 영재 학생의 교육용으로 특성화 학교 두 곳과 58개 영재교육원을 설립할 계획을 밝혔다고 한다.',\n",
       "  'The Korea Times reports to implement the plan, the ministry will set up two more specialized schools and 58 institutions specially designed for gifted students.'),\n",
       " ('군은 이라크 북부 무크다디야에서 군사작전 수행 중 이 시설을 발견했다.',\n",
       "  'The troops were conducting an operation north of Muqdadiya, Iraq, when they made the discovery.'),\n",
       " (\"15번 홀에서도 계속 세차례나 퍼팅을 했던 미셸 위는 ''그 홀이 내가 무너지게 된 원인이었다’’고 말했다.\",\n",
       "  \"``That just brought me down,'' said Wie, who went on to 3-putt the 15th hole, too.\"),\n",
       " ('탠디 레딕 조지아주 복권회사 대변인은 \"네이버는 1억1655만7083달러(약 1845억원)에 당첨됐으며 세금을 제한 실수령액은 약 8000만달러(약 760억원)가 될 것\"이라고 말했다.',\n",
       "  'He will net about $80 million after taxes, Georgia Lottery Corp. spokeswoman Tandi Reddick said.'),\n",
       " ('힐 대표의 안 좋은 건강은 일부 여성 기자들의 모성 본능을 불러일으키기도 했다.',\n",
       "  \"Concerns about Hill's health he has been sick several times in the cold Beijing winter and looked ill several days ago have elicited almost motherly concern from some female reporters.\"),\n",
       " ('디들리는 1950년대와 1960년대 척 베리, 팻츠 도미노, 엘비스 프레슬리와 동등한 인기를 누리며 이들과 비슷한 음악을 들려줬다.',\n",
       "  'Jack Jordan, 37, will serve the probationary term in his home state of Maryland with a stipulation that he obtain outpatient psychiatric treatment, spokesperson Tracy Golden confirmed.'),\n",
       " ('NCAA 남자 농구 토너먼트의 1번 시드인 루이빌은 오늘, 정확히는 오늘 저녁 무어헤드를 상대로 첫 경기를 치릅니다.',\n",
       "  \"Top overall seed in the NCAA Men's Basketball Tournament sees its first action today, actually this evening, against Moorehead State.\"),\n",
       " ('federal 연방정부의, 연방의 loan 대출, 대부',\n",
       "  'GM and Chrysler say they need a total of $11 billion in federal loans this month.'),\n",
       " ('형식적인 것으로 보이는 투표에서 그가 유일한 후보였다는 것은 이해된다.',\n",
       "  'It is understood that he was the only nominee in a vote seen as a formality.'),\n",
       " ('아늑함을 느낄수 있는 작고 아름다운 완벽한 덴마크의 풍경이다.',\n",
       "  \"It's a perfect Danish scene where small is beautiful, sustainability is just common sense, and a favorite word, hyggelig, takes cozy to unknown extremes.\"),\n",
       " ('이 호텔은 웹사이트에서 \"무료 숙박권은 영국 호텔 체인 322곳 어디서든 사용 가능하며 부부는 신분증과 함께 오랜 관계를 유지해왔다는 증명서를 제출해야 한다\"고 말했다.',\n",
       "  \"The offer is good at any one of the chain's 322 hotels in the United Kingdom, the Web site says. The couples must bring proof of identity and must prove that they are in a long-term relationship.\"),\n",
       " ('다음달 7일 후임자인 드미트리 메드베데프 당선자에게 권좌를 물려주는 푸틴은 “두 나라간 가장 민감한 문제는 MD 기지 문제”라며 “미국의 계획에 대한 러시아의 기본적인 입장은 바뀌지 않았다”고 말했다.',\n",
       "  'I will not conceal that one of the most difficult issues was and remains missile defense in Europe,\" said Putin, who will be replaced by Russia\\'s president-elect Dmitry Medvedev next month. \"Our fundamental attitude toward the American plan has not changed.'),\n",
       " ('AKP는 정치에 개입한 지난 5년간 인권과 사법 개정 법안의 통과를 도왔다.',\n",
       "  \"During the past five years of AK Party government, Gul has championed Turkey's EU bid, and helped push through human rights reforms and judicial amendments.\"),\n",
       " ('2005 챔피언스리그에서 우승하고 지난해 대회에서 준우승을 차지했던 리버풀은 3주전 1차전에서 인터 밀란을 2-0으로 꺾은 뒤 이날 또 다시 승리, 2연승으로 8강에 올랐다.',\n",
       "  'Liverpool, who won the tournament in 2005 and lost in the final last year, advanced with a 3-0 aggregate after winning 2-0 at Anfield in the first leg three weeks ago.'),\n",
       " ('의료진은 당시 아이가 영양부족과 빈혈 그리고 심한 심장의 압박을 받고 있었다고 설명했다.',\n",
       "  'They found the child to be undernourished, anemic and under cardiac stress.'),\n",
       " ('이는 의도와 는 상관없이 그에 대한 많은 궁금증을 자아냈다.',\n",
       "  \"It makes people more curious about her the opposite of Zane's intent.\"),\n",
       " ('아담스 대통령은 철저히 노예제를 반대해 노예를 두지 않았다.',\n",
       "  'Adams was a staunch opponent of slavery, and kept no slaves.'),\n",
       " ('쿠나베스는 애리조나 투손에서 열린 화상회의에서 “화성의 토양은 유기체만 제외하면 지구와 유사한 면이 많다”고 밝혔다.',\n",
       "  '\"It\\'s very typical of the soil here on Earth minus the organics,\" Kounaves said during a teleconference from Tucson, Arizona.'),\n",
       " ('검은 연기가 하늘 위를 뒤덮고 앰뷸런스는 사이렌을 울리며 사고 현장으로 달렸다.',\n",
       "  'Thick black smoke billowed up into the sky and ambulances raced to the location with sirens wailing.'),\n",
       " ('이일은 1998년 23세의 스위스 근위병이 자신의 상관을 죽이고 자신도 자살을 택한 사건을 떠올리게 했다.',\n",
       "  'The incident rekindled memories of the 1998 murder-suicide behind the Vatican walls, in which a 23-year-old Swiss Guard allegedly killed his commander.'),\n",
       " ('치명적인 부상자는 없었지만 이로인해 다른 주에서 흑곰들이 포획되었고, 곰의 개체수가 상당히 줄어있었다.',\n",
       "  'But black bears have killed people in other states, and can inflict significant damage on crops and livestock.'),\n",
       " ('터키에서 19일(현지시간) 택시, 페리, 쇼핑몰 등 대부분의 공공장소에서 금연을 실시하는 흡연 금지법안이 발효됐다.',\n",
       "  'ANKARA, Turkey (CNN) A law extending a smoking ban in Turkey to most enclosed areas including taxis, ferries and shopping malls came into effect Monday in the nicotine-addicted nation.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2889\n",
      "2889\n",
      "Korean: ['후보작', '으로', '선정', '된', '부문', '의', '수', '는', '이브', '에', '관한', '모든', '것', '1950', '년', '과', '타이타닉', '1997', '년', '에', '뒤이', '어', '오스카', '역사', '상', '2', '번', '째', '로', '많', '은', '것', '이', '다', '.']\n",
      "English: ['<start>', 'the', 'number', 'of', 'nominations', 'is', 'the', 'second-most', 'after', 'the', '14', 'earned', 'by', 'all', 'about', 'eve', '1950', 'and', 'titanic', '1997', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "\n",
    "num_examples = len(corpus_list)\n",
    "\n",
    "for pair in corpus_list[:3500]:\n",
    "    ret_ko = preprocess_sentence(pair[0])\n",
    "    ret_en = preprocess_sentence(pair[1], s_token=True, e_token=True)\n",
    "    if len(ret_ko) <= 40:\n",
    "#    if len(ret_ko) <= 20:\n",
    "        kor_corpus.append(ret_ko)\n",
    "        eng_corpus.append(ret_en)\n",
    "\n",
    "print(len(kor_corpus))\n",
    "print(len(eng_corpus))\n",
    "print(\"Korean:\", kor_corpus[100])   \n",
    "print(\"English:\", eng_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean Vocab Size: 10324\n",
      "English Vocab Size: 10131\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "# 훈련 데이터와 검증 데이터로 분리하기\n",
    "\n",
    "enc_tensor, enc_tokenizer = tokenize(kor_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(eng_corpus)\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "print(\"Korean Vocab Size:\", len(enc_tokenizer.index_word))\n",
    "print(\"English Vocab Size:\", len(dec_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (1, 30, 1024)\n",
      "Decoder Output: (1, 10132)\n",
      "Decoder Hidden State: (1, 1024)\n",
      "Attention: (1, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "#BATCH_SIZE     = 64\n",
    "BATCH_SIZE     = 1\n",
    "SRC_VOCAB_SIZE =  len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "#SRC_VOCAB_SIZE =  10000 #len(enc_tokenizer.index_word) + 1\n",
    "#TGT_VOCAB_SIZE = 10000 #len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "#units         = 512\n",
    "embedding_dim = 512\n",
    "#embedding_dim = 128\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 2311/2311 [10:20<00:00,  3.73it/s, Loss 3.2825]\n",
      "Epoch  2: 100%|██████████| 2311/2311 [08:43<00:00,  4.41it/s, Loss 3.1298]\n",
      "Epoch  3: 100%|██████████| 2311/2311 [09:01<00:00,  4.26it/s, Loss 3.1395]\n",
      "Epoch  4: 100%|██████████| 2311/2311 [09:01<00:00,  4.27it/s, Loss 3.1122]\n",
      "Epoch  5:  28%|██▊       | 656/2311 [02:38<06:16,  4.40it/s, Loss 3.0359]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define eval_step\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "\n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "\n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "#    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "#    print(result)\n",
    "#    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    attention = attention[:len(result.split()), :len(sentence)]\n",
    "#    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "    plot_attention(attention, sentence, result.split(' '))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"오바마는 대통령이다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"커피는 필요 없다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
