{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- # 작사가 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 조금 특이하게 작사가 인공지능을 만들었다.  \n",
    "input data를 통해서 어떤 객체인지 판별하는 분류기가 아닌 새로운 산출물을 만들어내는  \n",
    "토이 프로젝트였기 때문에 조금 새로운 느낌이 들었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## 데이터 준비  \n",
    "하지만 역시나 그렇듯 input data는 존재한다. 파일을 변수에 담아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Now greetings to the world! Standing at this liquor store,', 'Whiskey coming through my pores,', 'Feeling like I run this whole block.']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "담아준 텍스트의 일정 부분을 출력해 보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0 or len(sentence) > 15: continue   # 길이가 0인 문장은 skip\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장 skip\n",
    "    if idx > 9: break   # 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate 함수를 통해 raw_corpus에 들어있는 텍스트의 기본적인 전처리를 해준다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바꾼다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 단계에서는 문장 전처리에 대한 함수를 정의했다. 위의 전처리 단계에서는 먼저 체로  \n",
    "학습할 데이터 들을 \n",
    "<strong>걸러 내는 듯한 느낌</strong>이었다면 이번 전처리에서는 데이터를 <strong>다루기 쉽게 만들어주는 단계</strong>이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like a serial killer, man is a goner (man cured) \n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus: \n",
    "    if len(sentence) == 0 or len(sentence) > 15: continue #문장의 길이가 0이거나\n",
    "                                                          #15 이상이라면 skip\n",
    "    if sentence[-1] == \":\":continue #문장의 끝이 :이라면 skip\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))#corpus에 전처리된 문장을 추가\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing  \n",
    "--------------------------------\n",
    "\n",
    "- ### 토큰화란?  \n",
    "코퍼스 데이터(말뭉치 - 사람들이 사용하는 자연어로 보면 된다!)가 필요에 맞게 전처리되지  \n",
    "않았을 때 입력데이터로 활용하고자 한다면 반드시 토큰화가 필요한데, token은 보통 단어나  \n",
    "어떤 부분이 의미를 가질 때 정해진다.  \n",
    "\n",
    "아래 코드는 토큰화를 해주는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    8   14 ...    0    0    0]\n",
      " [   2    8   14 ...    0    0    0]\n",
      " [   2    8   14 ...    0    0    0]\n",
      " ...\n",
      " [   2   72   26 ...    0    0    0]\n",
      " [   2  140    3 ...    0    0    0]\n",
      " [   2   78 3416 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f608312be90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    #전처리 로직을 따로 설정해주지 않을 것(커스터마이징이 가능하다)\n",
    "        oov_token=\"<unk>\"  # <unk> == unknown 사전에 없는 단어는 해당 토큰으로 대체된다.\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   #fit_on_text 함수를 통해서 사전을 자동구축할 수 있다.\n",
    "\n",
    "   #tokenizer를 통해서 입력할 수 있는 데이터로 만들어준다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   #text_to_sequeces는 corpus -> tensor의 역할을 한다.\n",
    "    # 입력 데이터의 시퀀스 길이를 맞춰주기 위해 padding 함수를 사용할 수 있다.\n",
    "    # maxlen의 디폴트값은 None인 경우 가장 긴 문장을 기준으로 시퀀스의 길이를 맞춰준다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit_on_text  \n",
    "주석으로는 '사전을 구축 한다'라는 바로 와닿지 않는 말을 사용했는데,  \n",
    "사실 이 함수는 문자 데이터를 입력받아서 리스트의 형태로 변환해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 14 14  3  0  0  0  0  0  0  0  0  0]\n",
      "[ 8 14 14  3  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 src_input은 마지막 토큰을 잘라내었기 때문에 < end >로 끝날 수 있다고 생각하기  \n",
    "쉽지만 위에서 padding 작업을 했기 때문에 < pad >로 끝날 가능성이 더 높다 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input) #buffer는 입력할 문장의 개수로 해주는 것이 당연!\n",
    "BATCH_SIZE = 256 \n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 \n",
    "                                        #<pad>를 포함하여 7001개\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from_tensor_slices()??  \n",
    "----------------------------------------  \n",
    "tf.data.Dataset에서의 from_tensor_slices()는 <strong><span style = \"color  : blue\">주어진 입력 텐서의 첫번째 차원을 따라 슬라이스</span></strong> 해준다. 이때 모든 입력 텐서들은 첫번째 차원과 같은 크기를 가져야한다고 한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 직접적으로 텍스트를 만들어줄 text Generator 클래스(모델)를 만들어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size): #초기화 함수에\n",
    "                                                                #각각 레이어를 달아준다\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 #워드 벡터의 차원수(단어가 추상적으로 표현되는 크기. 값이 커질수록\n",
    "                    #단어의 추상적인 특징들을 더 잘 잡아내는 반면 이에 맞는 데이터가 주어\n",
    "                    #지지 않는다면 오히려 그 특징들을 잡아낼 수 없을 것이다.)\n",
    "hidden_size = 1024\n",
    "#모델에 얼마나 많은 손을 달아줄 것인가? embedding_size와 비슷하게 크기에 맞는 데이터가\n",
    "#주어지지 않는다면 허공에 손을 휘젓는 꼴이 될 것이다\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 7001), dtype=float32, numpy=\n",
       "array([[[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [ 6.88346845e-05, -8.85950343e-04,  2.21441122e-04, ...,\n",
       "         -6.60828431e-04, -3.76987387e-04,  1.04010804e-04],\n",
       "        [ 1.49475760e-04, -1.18420378e-03,  2.00043447e-04, ...,\n",
       "         -6.30448456e-04, -3.13722121e-04,  4.13638336e-05],\n",
       "        ...,\n",
       "        [-5.85275178e-04, -1.01008639e-03,  9.90796951e-04, ...,\n",
       "          2.05876119e-03,  8.42770794e-04,  3.85059649e-03],\n",
       "        [-7.00698583e-04, -1.03843352e-03,  1.09440170e-03, ...,\n",
       "          2.15463340e-03,  9.07822687e-04,  3.93811474e-03],\n",
       "        [-8.02449125e-04, -1.07970694e-03,  1.18517282e-03, ...,\n",
       "          2.20872555e-03,  9.57608398e-04,  3.96862207e-03]],\n",
       "\n",
       "       [[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [ 3.07370647e-04, -4.52753011e-04, -3.19276558e-04, ...,\n",
       "         -2.64787668e-04, -1.86993726e-04, -1.85593351e-04],\n",
       "        [ 5.84631518e-04, -2.36209336e-04,  3.85826206e-05, ...,\n",
       "         -2.73980724e-04, -5.26175718e-04, -2.24383140e-04],\n",
       "        ...,\n",
       "        [-1.02939317e-03, -5.70531934e-04,  1.16661715e-03, ...,\n",
       "          2.46340781e-03,  5.73888188e-04,  3.79854883e-03],\n",
       "        [-1.15369027e-03, -6.60678954e-04,  1.22965500e-03, ...,\n",
       "          2.51784758e-03,  6.86905812e-04,  3.91200464e-03],\n",
       "        [-1.24766864e-03, -7.53658474e-04,  1.28536450e-03, ...,\n",
       "          2.53005140e-03,  7.80189759e-04,  3.96384811e-03]],\n",
       "\n",
       "       [[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [-1.88909005e-04, -6.29940536e-04,  1.76842150e-06, ...,\n",
       "         -1.70169878e-04, -3.22490698e-04,  3.71442962e-04],\n",
       "        [-1.00738202e-04, -5.44587034e-04,  5.68068644e-05, ...,\n",
       "         -5.18901972e-04, -3.80492187e-04,  5.17904642e-04],\n",
       "        ...,\n",
       "        [-5.12152503e-04, -9.14261444e-04,  9.45978740e-04, ...,\n",
       "          2.35453318e-03,  9.04625049e-04,  3.94584658e-03],\n",
       "        [-6.30033552e-04, -1.00155128e-03,  1.03447947e-03, ...,\n",
       "          2.45458516e-03,  9.75601724e-04,  4.00267681e-03],\n",
       "        [-7.35180627e-04, -1.08625123e-03,  1.11486972e-03, ...,\n",
       "          2.50315736e-03,  1.02956838e-03,  4.00976604e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [ 2.59756518e-04, -6.45839842e-04,  2.49907316e-04, ...,\n",
       "         -1.35705821e-04, -3.37018661e-04,  2.26194446e-04],\n",
       "        [ 3.37134406e-04, -7.62720592e-04,  5.25937648e-05, ...,\n",
       "         -3.86995205e-04, -7.36753165e-04,  3.90517642e-04],\n",
       "        ...,\n",
       "        [-3.34306271e-04, -8.39339453e-04,  8.69766343e-04, ...,\n",
       "          2.20894325e-03,  5.62250265e-04,  3.73602239e-03],\n",
       "        [-4.80244053e-04, -9.19512939e-04,  9.78496391e-04, ...,\n",
       "          2.29230244e-03,  6.77007716e-04,  3.82841984e-03],\n",
       "        [-6.09916111e-04, -1.00129878e-03,  1.07325031e-03, ...,\n",
       "          2.33157771e-03,  7.68506259e-04,  3.86993890e-03]],\n",
       "\n",
       "       [[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [ 2.31552083e-04, -6.74253621e-04,  1.15721348e-04, ...,\n",
       "          1.59587202e-04,  9.72991256e-05,  2.18495450e-04],\n",
       "        [ 2.21611699e-04, -1.03755982e-03,  2.34520048e-04, ...,\n",
       "          4.68173414e-04,  2.25531694e-04,  1.33807800e-04],\n",
       "        ...,\n",
       "        [-9.83078033e-04, -1.02563831e-03,  1.12728716e-03, ...,\n",
       "          2.34145694e-03,  8.16299231e-04,  3.02792480e-03],\n",
       "        [-1.05749397e-03, -1.06030179e-03,  1.20001426e-03, ...,\n",
       "          2.37419270e-03,  8.21937050e-04,  3.27751506e-03],\n",
       "        [-1.11691770e-03, -1.10440247e-03,  1.26035465e-03, ...,\n",
       "          2.37814058e-03,  8.33964907e-04,  3.45200673e-03]],\n",
       "\n",
       "       [[ 9.56580916e-05, -3.37890058e-04,  1.29056207e-05, ...,\n",
       "         -1.50304171e-04, -1.53379762e-04,  4.73160026e-06],\n",
       "        [-1.74394634e-04, -9.49090900e-05,  1.10053756e-04, ...,\n",
       "         -3.91729234e-04, -7.68585713e-04,  1.64890458e-04],\n",
       "        [ 9.16467252e-05, -8.25778989e-05,  5.83283691e-05, ...,\n",
       "         -5.52638026e-04, -1.50594371e-03,  1.55430520e-04],\n",
       "        ...,\n",
       "        [-3.62745806e-04, -2.53902632e-04,  7.27441977e-04, ...,\n",
       "          1.49196305e-03, -6.96707866e-04,  3.40797007e-03],\n",
       "        [-4.86558449e-04, -4.20555705e-04,  8.30198987e-04, ...,\n",
       "          1.73638249e-03, -3.89239052e-04,  3.57637834e-03],\n",
       "        [-6.06803806e-04, -5.72607620e-04,  9.22214240e-04, ...,\n",
       "          1.92129565e-03, -1.22649595e-04,  3.68217519e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드로 나온 텐서의 shape는 256, 14, 7001의 값을 갖는다.  \n",
    "256은 위에서 정해준 batch size, 14는 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미를 가지고, 7001은 익숙하게도 pad를 포함한 단어사전의 크기이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 확인  \n",
    "모델 구성을 완료했기 때문에 원하는 형태로 모델이 만들어졌는지 확인해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 95s 2s/step - loss: 2.3591\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 106s 2s/step - loss: 1.3731\n",
      "Epoch 3/30\n",
      "34/55 [=================>............] - ETA: 53s - loss: 1.2398"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 생성\n",
    "학습까지 완료된 모델을 통해서 텍스트를 만들어줄 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 우선 입력받은 단어를 텐서로 변환해준다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 루프를 돌면서 단어 하나씩 생성. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력 \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  \n",
    " \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        #<end>를 예측하거나 maxlen에 다다를때 까지 루프 반복\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 문장 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고  \n",
    "새로운 산출물을 내는 모델은 새로웠다. 사실 예측이라는 측면에서 분류기 모델들과  \n",
    "크게 다르지 않다는 생각이 들었고, 너무 복잡하게 생각할 필요도 없다는 생각이 들었다.  \n",
    "언어학을 전공해서 크게 어렵지 않겠다는 생각을 가지고 시작한 토이프로젝트 였지만, 사실  \n",
    "코퍼스나 진폭, 주파수(음성 분석) 등 용어만 익숙하고 개념 자체는 오히려 더 어색했다.  \n",
    "해당 프로젝트를 진행하면서 모델이 어떻게 문법을 예측하는지, 적절한 단어를 고르는지에 대한  \n",
    "모든 인사이트는 들어오지 않았지만, 어렴풋이 감을 잡을 수 있는 토이프로젝트 였던 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
